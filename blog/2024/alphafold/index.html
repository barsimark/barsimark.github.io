<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Highly accurate protein structure prediction with AlphaFold | Mark Barsi </title> <meta name="author" content="Mark Barsi"> <meta name="description" content="Blogpost based on Highly accurate protein structure prediction with AlphaFold article by Nature"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://barsimark.github.io/blog/2024/alphafold/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Highly accurate protein structure prediction with AlphaFold",
            "description": "Blogpost based on Highly accurate protein structure prediction with AlphaFold article by Nature",
            "published": "July 10, 2024",
            "authors": [
              
              {
                "author": "Mark Barsi",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "University of Stuttgart",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mark</span> Barsi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Highly accurate protein structure prediction with AlphaFold</h1> <p>Blogpost based on Highly accurate protein structure prediction with AlphaFold article by Nature</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#short-summary">Short summary</a> </div> <div> <a href="#purpose">Purpose</a> </div> <div> <a href="#protein-structure">Protein structure</a> </div> <div> <a href="#dataset">Dataset</a> </div> <div> <a href="#model">Model</a> </div> <ul> <li> <a href="#evoformer-blocks">Evoformer blocks</a> </li> <li> <a href="#structure-blocks">Structure blocks</a> </li> <li> <a href="#training">Training</a> </li> </ul> <div> <a href="#results">Results</a> </div> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <h2 id="short-summary">Short summary</h2> <p>Protein structures are fundamental to biological research, yet the known protein structures represent a tiny fraction of the vast diversity in existence. Traditional methods for determining protein structures through computational means, such as physical interactions and evolutionary history, are complex and time-consuming. To address these challenges, AlphaFold, a neural network model, was introduced in 2021, offering near-experimental accuracy in predicting three-dimensional protein structures. AlphaFold leverages a two-part architecture: Evoformer blocks and structure modules. Evoformer blocks predict relationships between protein components using graph inference, while structure modules calculate explicit three-dimensional structures through iterative refinement. The model is trained using the Protein Data Bank and self-distillation datasets, with additional genetic and structure databases for multiple sequence alignments. AlphaFold’s training involves a two-stage process with both initial and fine-tuning phases, achieving impressive accuracy. Results demonstrate the model’s capability in predicting both simple and complex protein structures with high precision. The AlphaFold model’s public availability has spurred further research, with significant implications for medical and biological sciences, such as developing vaccines, understanding genetic diseases, and combating antibiotic resistance. Since its publication, AlphaFold has been highly influential, evidenced by numerous citations and widespread recognition in the scientific community.</p> <h2 id="purpose">Purpose</h2> <p>Protein is essential in biology since it is the basis of life. Despite all kinds of various different research being conducted in this field, the number of known protein structures is relatively low. Only about one hundred thousand proteins have a structure known to science, out of potentially millions or billions of existing proteins.</p> <p>There are two existing traditional ways of finding the structure of protein using computational methods: physical interaction, and evolutionary history. The first one integrates our understanding of molecular physics, thermodynamics, and kinetic simulations; while the second one is derived from the history of protein evolution. In theory, both techniques sound like a great way to calculate three-dimensional protein structures, however, in practice they are rarely used as they are rather challenging even for small protein, they are highly dependent on context, they are not very accurate, and they take a really long time to do. Since the traditional way is not appealing to calculate protein structure, a new approach was proposed utilizing neural networks to directly get the accurate three-dimensional structures to near experimental accuracy in the vast majority of the cases. This is exactly the reason why the AlphaFold network was created in 2021.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/casp14_predictions-480.webp 480w,/assets/img/alphafold/casp14_predictions-800.webp 800w,/assets/img/alphafold/casp14_predictions-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/casp14_predictions.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The near-perfect accuracy of the AlphaFold model illustrated on two proteins (T1037 and T1049) </div> <p>The green structure is the experimental result, in other words the ground truth. As it can be seen on this comparison, the predicted or calculated result of AlphaFold – the blue structure – is almost the same for the main components, there are only a handful of errors in the side-chains.</p> <h2 id="protein-structure">Protein structure</h2> <p>In order to fully comprehend the neural network in the later chapters, a bit of background information about the structure of proteins is required. There are four components in these structures, in the following complexity order: primary, secondary, tertiary, and quaternary.</p> <p>The most basic structure is the primary. It is determined by the linear sequence of the components, the amino acids. There are 20 neutral ones, and a few ambigous mixtures. These amino acids form long chains by connecting to each other, resulting in the primary protein structure. The two ends of the protein chain are called the C-terminus and the N-terminus (Sanger, 1952).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/primary_structure-480.webp 480w,/assets/img/alphafold/primary_structure-800.webp 800w,/assets/img/alphafold/primary_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/primary_structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Primary protein structure (Creative Biostructure, 2024) </div> <p>The secondary structure refers to the local substructure of these proteins. This is also called the backbone. By definition, this is a “continuous chain of atoms that runs throughout the length of a protein” (Si, 2020) There are two different types of secondary structure: alpha-helix, and beta-sheets (Khan Academy, 2024) as illustrated by the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/secondary_structure-480.webp 480w,/assets/img/alphafold/secondary_structure-800.webp 800w,/assets/img/alphafold/secondary_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/secondary_structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Secondary protein structure (Khan Academy, 2024) </div> <p>The three dimensional shape of a single protein chain is called the tertiary structure. This is determined by the bonds within the chain (Khan Academy, 2024).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/tertiary_structure-480.webp 480w,/assets/img/alphafold/tertiary_structure-800.webp 800w,/assets/img/alphafold/tertiary_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/tertiary_structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Tertiary protein structure (Khan Academy, 2024) </div> <p>The final, quaternary structure is determined by the aggregation of various different chains. The bond between these individual chains gives the final structure of the protein, as showcased by this image (Wikipedia, 2024):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/quarternary_structure-480.webp 480w,/assets/img/alphafold/quarternary_structure-800.webp 800w,/assets/img/alphafold/quarternary_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/quarternary_structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Quaternary protein structure (Creative Biostructure, 2024) </div> <h2 id="dataset">Dataset</h2> <p>For input, the model mainly uses the so called Protein Data Bank (PDB) database. This contains the amino acid sequences for the protein, the coordinates of all components of the protein chains, as well as additional metadata that might be relevant for the structures. There are thousands of proteins in this database that were all used for training the AlphaFold network. The size of this dataset is about 2.62TB.</p> <p>This dataset alone however, wasn’t enough for training the neural network; even more data was necessary. Therefore, a so-called self-distillation dataset was created from the existing resources using different augmentation techniques such as chopping or MSA subsampling. After filtering out the unfeasible structures, about 350.000 new protein structures were created.</p> <p>A combination of these two datasets were used in the final training, which consisted of 75% self-distillation and 25% Protein Data Bank data.</p> <p>Two additional databases were also used for both training and inference: a genetic database, and a structure database. The genetic database was used for getting Multiple Sequence Alignments (MSA). According to the Multiple Sequence Alignment Wikipedia page, MSA is “the process or the result of sequence alignment of three or more biological sequences, generally protein, DNA, or RNA. These alignments are used to infer evolutionary relationships via phylogenetic analysis and can highlight homologous features between sequences.” (Wikipedia, 2024). The second, structure database was required for helping the model find the exact, correct three-dimensional structure of the given protein.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/msa-480.webp 480w,/assets/img/alphafold/msa-800.webp 800w,/assets/img/alphafold/msa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/msa.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> An example for Multiple Sequence Alignment (Biorender, 2024) </div> <p>For accuracy assessment, the so-called CASP14 assessment was used, which is the gold-standard for protein structure accuracy prediction.</p> <h2 id="model">Model</h2> <p>The AlphaFold network was built using two components: the first part consisting of Evoformer blocks, and the second of structure modules.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/architecture-480.webp 480w,/assets/img/alphafold/architecture-800.webp 800w,/assets/img/alphafold/architecture-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The full architecture of AlphaFold </div> <p>The model has a Recurrent Neural Network (RNN) architecture, as illustrated by the recycling step in the bottom of the chart. It allows the network to constantly refine the results by reconnecting the output of both key components (Evoformer and structure modules) of the network back to the beginning three times.</p> <p>In my opinion, this is one of the new innovations of the network that makes the results so accurate, and probably why the model architecture inspired so many other researchers in numerous different fields.</p> <p>Lets look at the Evoformer and structure modules a bit more detailed.</p> <h3 id="evoformer-blocks">Evoformer blocks</h3> <p>The Evoformer blocks make up the first half of the neural network. They are responsible for finding the correct protein components, as well as the connections between these components.</p> <p>The core idea behind these modules is to predict the connections between the components by interpreting the protein connections as a graph inference problem in three-dimensional space. The nodes represent the individual residues, while the edges are the proximity of residues.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/pair_representation-480.webp 480w,/assets/img/alphafold/pair_representation-800.webp 800w,/assets/img/alphafold/pair_representation-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/pair_representation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> A simple example for the connection between pair representation and graph inference </div> <p>The Evoformer blocks have two input: an MSA representation and a so-called pair representation. The MSA representation is a matrix where the columns represent the individual residues, and the rows are sequences where the said residues appear. Therefore, the dimensions of this input is Nseq x Nres, the number of sequences times number of residues. The second input is the pair representation matrix, which contains the relationship between the residues. This matrix has a shape of Nres x Nres.</p> <p>The output of these blocks is a refined version of both the MSA representation and the pair representation. The dimension of these are the same as the input’s which allows the blocks to be chained without any modification.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/evoformer-480.webp 480w,/assets/img/alphafold/evoformer-800.webp 800w,/assets/img/alphafold/evoformer-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/evoformer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The architecture of the Evoformer blocks </div> <p>An individual Evoformer block contains both attention-based and non-attention-based modules. The combination of these allow the model to focus on details and overview at the same time. During the training, the MSA representation gets refined first using self-attention modules. The refined MSA matrix is than used to refine the pair-representation matrix with the help of a few additional self-attention and triangle update nodes.</p> <p>In order to avoid, or at least reduce the impact of overfitting, two methods were used in the Evoormer blocks. As it can be seen on the architecture image, there are skip-connections around all the building blocks within the Evoformer. These allow information flow around the blocks or layers, which as a result avoid the vanishing gradient problem, hence reducing overfitting. In addition, dropout layers were used to further decrease overfitting.</p> <p>Overall, there are 48 of these Evoformer blocks in the network with individual weights, meaning there are no shared weights between the different modules.</p> <h3 id="structure-blocks">Structure blocks</h3> <p>The second part of the neural network consists of eight structure modules that are responsible for predicting, calculating the explicit three-dimensional structure of the proteins.</p> <p>The concept of the structure blocks is fairly straight forward. Only two matrixes need to be calculated in order to have a properly defined, explicit three-dimensional structure: a rotation and translation for each of the consisting residues. When these are defined, the concrete protein structure will be defined. Both of them will be iteratively calculated during the training process. Therefore, their starting values need to be initialized.</p> <p>For both matrixes, the trivial initialization was used. The rotation matrixes are set to be the identity matrix, meaning that none of the residues are rotated, so all of them are facing the same direction. The translation matrixes are all set in a way that all of the residues start from the origin, the center. This way of initialization allows the training process to have minimal bias coming from the starting values, which is always a key challenge of neural networks.</p> <p>Two inputs are needed for these calculations, both of them are coming from the previous Evoformer blocks: the final, refined MSA representation, as well as the pair representation matrix. With the help of these inputs, the structure block then calculates the relative rotation and translation for all the residues. It is very important to note, that these are relative values, relative to the other residues. At the end of the day it doesn’t matter if the whole structure is shifted to one side or the other, or if the whole thing is rotated; it is still the same structure. The output of these modules is the explicit three-dimensional structure, as well as the MSA and pair representation to serialize the modules.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/structure-480.webp 480w,/assets/img/alphafold/structure-800.webp 800w,/assets/img/alphafold/structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The architecture of the structure modules </div> <p>In these modules, the so-called Frame-aligned Point Error function was used. This function calculates the distance between the predicted position and the actual, correct position for all the residues. The sum of all these is the final loss value. This method creates a strong bias for the residues to be in the correct location relative to the local frame, relative to the other residues.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/fape_loss-480.webp 480w,/assets/img/alphafold/fape_loss-800.webp 800w,/assets/img/alphafold/fape_loss-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/fape_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> A simple example for the FAPE loss </div> <p>There are eight of these structure modules, with shared weights between them. Meaning that all of the blocks have the same weight matrix.</p> <h3 id="training">Training</h3> <p>The training of the AlphaFold network consists of two stages, a first initial training followed by a fine-tuning step.</p> <p>For the base version, only the PDB dataset was used. The weight initialization was done by the highly popular He initializer. Since the training at this step was started from scratch, a relatively large learning rate was needed, therefore it was set to 10-3. For the same reason, the number of training samples was also required to be high, about ten million samples were used. The total base training of the model takes 7 days.</p> <p>The second training step is the fine-tuning, where only smaller adjustments are done in order to increase the overall accuracy. For this, the previously introduced self-distillation data was utilized in addition to the basic PDB set. Since only minor corrections were done, the learning rate was set to a smaller number, 5*10-4. The number of samples needed was also lower, only about 1.5 million of them were used. The fine-tuning takes about 4 days of training time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/parameters-480.webp 480w,/assets/img/alphafold/parameters-800.webp 800w,/assets/img/alphafold/parameters-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/parameters.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Extended overview of the model parameters </div> <p>As for optimizer, the most popular Adam optimizer was used.</p> <p>The code of the AlphaFold network is open-source, available on GitHub (https://github.com/google-deepmind/alphafold). Just to illustrate how big this network is, these are the official recommended hardware parameters for running the model in Google Cloud: 12 vCPU, 85GB RAM, 100GB boot disk, 3TB space for the datasets, and an Nvidia A100 Tensor Core GPU with 80GB GPU memory (NVidia, 2024).</p> <h2 id="results">Results</h2> <p>The AlphaFold network produces really impressive results on smaller as well as larger protein structures. In this chapter, the results will be discussed in a bit more details. For this, the metrics are divided into two parts: backbone that is the core structure and all-atom which consists of the backbone and the so-called side-chains. Obviously, the all-atom accuracy is always going to be lower, since it is much more difficult to predict the side-chains than the backbone.</p> <p>The total backbone accuracy of the model using root mean square deviation at 95% coverage is 0.96 Å (Å: angstrom = 0.1 nanometer (Britannica, 2023)). For the 95% confidence interval, the error is in the range of 0.85 – 1.16 Å. For all atoms, with the same coverage, the root mean square metric is 1.5 Å, the interval is 1.2 – 1.6 Å. That level of accuracy is outstanding, especially considering that the width of a single carbon atom is approximately 1.4 Å.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/errors-480.webp 480w,/assets/img/alphafold/errors-800.webp 800w,/assets/img/alphafold/errors-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/errors.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Total error distribution of the model. First row shows the 100%, the second the 95% coverage. First column is the backbone, the second is the all-atom accuracy. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/error_chart-480.webp 480w,/assets/img/alphafold/error_chart-800.webp 800w,/assets/img/alphafold/error_chart-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/error_chart.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The quantiles of error distribution for both the backbone and all-atom </div> <p>The following video shows the training process of the Evoformer blocks during the three recycling phases for a small and simple protein of CASP14 target T1024 (LmrP). This protein consists of 408 residues. It can be seen how early in the process the model finds the correct structure, as well as the minor adjustments of the later steps.</p> <figure> <video src="/assets/video/alphafold/41586_2021_3819_MOESM3_ESM.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <p>The second video is of a larger but still simple protein of CASP14 target T1044 (RNA polymerase of crAss-like phage). This structure consists of 2180 residues. It takes a bit longer to find the final structure and to find the correct relative positions.</p> <figure> <video src="/assets/video/alphafold/41586_2021_3819_MOESM4_ESM.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <p>The third video illustrates the training steps of a large and complex protein, namely CASP14 target T1091 which consists of 863 residues. The individual structures are determined early on, however there are few residues which it fails to place correctly during the entire training. This results in the visible unphysical strings on the video.</p> <figure> <video src="/assets/video/alphafold/41586_2021_3819_MOESM6_ESM.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <h2 id="discussion">Discussion</h2> <p>During the training of the 48 Evoformer blocks, intermediate structure trajectories are calculated. These trajectories represent the networks belief of the most likely structure. Given that these trajectories remain smooth during the entire training, means that the model incrementally improves the structure until it can no longer be improved anymore, so when the final correct structure is found. This leads to the network being highly-accurate for both the backbone and the side-chain structure predictions.</p> <p>As evidenced by the previous videos, the model can cope with both simplicity and complexity, which is often a challenge of neural networks. For easy structures, like the T1024 (LmrP), the solution is found in the first couple of layers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/lmrp_structure-480.webp 480w,/assets/img/alphafold/lmrp_structure-800.webp 800w,/assets/img/alphafold/lmrp_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/lmrp_structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The model's output for the simple T1024 (LmrP) protein (PDB, 2020) </div> <p>For challenging, complex structures, for example the T1091 (on the third video) or the ORF8 of SARS-CoV-2 (T1064) the model constantly searches and rearranges until it can no longer be improved, or the training process is over. The extremely complex structure of the latter protein can be seen here:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alphafold/sarscov2_structure-480.webp 480w,/assets/img/alphafold/sarscov2_structure-800.webp 800w,/assets/img/alphafold/sarscov2_structure-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/alphafold/sarscov2_structure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The extremely complex structure of the SARS-CoV-2 protein (Vinjamoir, 2022) </div> <p>The direct, explicit coordinate output of the model makes it convenient and easy to use for various different scenarios in practice. Combining this with the speed of the predictions, which is about one GPU minute for a model of 384 residues, the model is extremely capable of helping researchers in all fields of science.</p> <p>The architecture alongside the full source and pseudo codes are available for the public, therefore it can also inspire further development and innovation in the field of artificial intelligence and neural networks.</p> <p>The research impact of the paper is already outstanding. The article was published in Nature in 2021, and has already made a huge impact on the scientific community. As of July 2024, the article already has 1.6 million views, and more than 22 thousand citations. Out of similar aged articles, it is ranked at 105th out of all the 450.000 articles (99th percentile) tracked across all journals, and 15th out of all Nature articles.</p> <p>Just a few examples of what the scientific community thinks is possible to do with artificial intelligence that is inspired by the AlphaFold architecture:</p> <ul> <li> <p>Stopping the spread of malaria by developing a vaccine that can potentially save hundreds of thousands of lives every year (Google Deepmind, 2022)</p> </li> <li> <p>Fighting osteoporosis before the bones start to break by understanding genetic causes through AlphaFold (Google Deepmind, 2022)</p> </li> <li> <p>Getting a better understanding of cancer and autism through faulty proteins (Google Deepmind, 2022)</p> </li> <li> <p>Finding a treatment to Parkinson’s, helping more than 10 million people worldwide (Google Deepmind, 2022)</p> </li> <li> <p>Beating antibiotic resistance to eliminate bacterial infections (Google Deepmind, 2022)</p> </li> </ul> <p>There are probably countless more problems that can potentially be solved with the help of AlphaFold, or an improved, even better version of the model (there is already an AlphaFold 2 release (LEWIS, 2022)).</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"barsimark/barsimark.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Mark Barsi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>